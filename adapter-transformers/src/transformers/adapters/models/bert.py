import logging
from typing import Union, List, Dict

import torch

from ..configuration import AdapterSwitchConfig
from ..composition import AdapterCompositionBlock, parse_composition
from ..heads import (
    BertStyleMaskedLMHead,
    BiaffineParsingHead,
    CausalLMHead,
    ClassificationHead,
    ModelWithFlexibleHeadsAdaptersMixin,
    MultiLabelClassificationHead,
    MultipleChoiceHead,
    QuestionAnsweringHead,
    TaggingHead,
)
from ..layer import AdapterLayerBaseMixin
from ..model_mixin import InvertibleAdaptersMixin, ModelAdaptersMixin


logger = logging.getLogger(__name__)


class BertSelfOutputAdaptersMixin(AdapterLayerBaseMixin):
    """Adds adapters to the BertSelfOutput module."""

    @property
    def adapter_config_key(self):
        return "mh_adapter"


class BertOutputAdaptersMixin(AdapterLayerBaseMixin):
    """Adds adapters to the BertOutput module."""

    @property
    def adapter_config_key(self):
        return "output_adapter"


class BertLayerAdaptersMixin:
    """Adds adapters to the BertLayer module."""


    def add_switch(self, adapter_names, layer_idx: int):
        self.attention.output.add_switch(adapter_names, layer_idx)
        self.output.add_switch(adapter_names, layer_idx)

    def add_fusion_layer(self, adapter_names):
        self.attention.output.add_fusion_layer(adapter_names)
        self.output.add_fusion_layer(adapter_names)

    def add_adapter(self, adapter_name: str, layer_idx: int):
        self.attention.output.add_adapter(adapter_name, layer_idx)
        self.output.add_adapter(adapter_name, layer_idx)

    def delete_adapter(self, adapter_name):
        self.attention.output.delete_adapter(adapter_name)
        self.output.delete_adapter(adapter_name)

    def delete_fusion_layer(self, adapter_names):
        self.attention.output.delete_fusion_layer(adapter_names)
        self.output.delete_fusion_layer(adapter_names)

    def delete_switch_layer(self, adapter_names):
        self.attention.output.delete_switch_layer(adapter_names)
        self.output.delete_switch_layer(adapter_names)

    def enable_adapters(
            self,
            adapter_setup: AdapterCompositionBlock,
            unfreeze_adapters: bool,
            unfreeze_attention: bool
    ):
        self.attention.output.enable_adapters(
            adapter_setup, unfreeze_adapters, unfreeze_attention
        )

        self.output.enable_adapters(
            adapter_setup, unfreeze_adapters, unfreeze_attention
        )

    def enable_switch(self, adapter_setup):
        self.attention.output.enable_switch(adapter_setup)
        self.output.enable_switch(adapter_setup)


class BertEncoderAdaptersMixin:
    """Adds adapters to the BertEncoder module."""


    def add_switch_layer(self, adapter_names):
        logger.info(f"BERT: add switch {adapter_names}.")

        # Get the configuration of the switch.
        switch_config: AdapterSwitchConfig
        switch_config = self.config.adapters.get_switch(adapter_names)
        leave_out = switch_config.get("leave_out", [])

        for i, layer in enumerate(self.layer):
            if i not in leave_out:
                layer.add_switch(adapter_names, i)

    def add_fusion_layer(self, adapter_names):
        for layer in self.layer:
            layer.add_fusion_layer(adapter_names)

    def add_adapter(self, adapter_name: str):
        logger.info(f"BERT: add adapter {adapter_name}.")
        adapter_config = self.config.adapters.get(adapter_name)
        leave_out = adapter_config.get("leave_out", [])
        for i, layer in enumerate(self.layer):
            if i not in leave_out:
                layer.add_adapter(adapter_name, i)

    def delete_adapter(self, adapter_name: str):
        for layer in self.layer:
            layer.delete_adapter(adapter_name)

    def delete_fusion_layer(self, adapter_names):
        for layer in self.layer:
            layer.delete_fusion_layer(adapter_names)

    def delete_switch_layer(self, adapter_names):
        for layer in self.layer:
            layer.delete_switch_layer(adapter_names)

    def enable_switch(self, adapter_setup):
        for idx, layer in enumerate(self.layer):
            layer.enable_switch(adapter_setup)

    def enable_adapters(
        self,
        adapter_setup: AdapterCompositionBlock,
        unfreeze_adapters: bool,
        unfreeze_attention: bool,
    ):
        for layer in self.layer:
            layer.enable_adapters(
                adapter_setup, unfreeze_adapters, unfreeze_attention
            )

    def adjust_attention_mask_for_parallel(self, hidden_states, attention_mask):
        if attention_mask is not None and hidden_states.shape[0] != attention_mask.shape[0]:
            repeats = [1] * len(attention_mask.shape)
            repeats[0] = hidden_states.shape[0] // attention_mask.shape[0]
            attention_mask = attention_mask.repeat(*repeats)
        return attention_mask


class BertModelAdaptersMixin(InvertibleAdaptersMixin, ModelAdaptersMixin):
    """Adds adapters to the BertModel module."""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def train_adapter(self, adapter_setup: Union[list, AdapterCompositionBlock]):
        """Sets the model into mode for training the given adapters."""
        self.train()
        self.freeze_model(True)
        adapter_setup = parse_composition(adapter_setup)
        self.encoder.enable_adapters(adapter_setup, True, False)
        self.enable_invertible_adapters(adapter_setup.flatten())
        # use the adapters to be trained by default in every forward pass
        self.set_active_adapters(adapter_setup)

    def train_adapter_fusion(
        self, adapter_setup: Union[list, AdapterCompositionBlock],
        unfreeze_adapters=False
    ):
        # Sets the model into mode for training of adapter fusion determined
        # by a list of adapter names.

        self.train()
        self.freeze_model(True)
        adapter_setup = parse_composition(adapter_setup)
        self.encoder.enable_adapters(adapter_setup, unfreeze_adapters, True)
        # use the adapters to be trained by default in every forward pass
        self.set_active_adapters(adapter_setup)
        # TODO implement fusion for invertible adapters

    def train_adapter_switch(self, adapter_setup: AdapterCompositionBlock):
        # Sets the model into mode for training of adapter fusion determined
        # by a list of adapter names.
        self.train()
        self.freeze_model(True)
        adapter_setup = parse_composition(adapter_setup)
        self.encoder.enable_switch(adapter_setup)
        self.set_active_adapters(adapter_setup)

    def _add_adapter(self, adapter_name):
        self.encoder.add_adapter(adapter_name)
        self.add_invertible_adapter(adapter_name)

    def _add_switch_layer(self, adapter_names):
        self.encoder.add_switch_layer(adapter_names)

    def _add_fusion_layer(self, adapter_names):
        self.encoder.add_fusion_layer(adapter_names)

    def _delete_adapter(self, adapter_name: str):
        self.encoder.delete_adapter(adapter_name)
        self.delete_invertible_adapter(adapter_name)

    def _delete_fusion_layer(self, adapter_names):
        self.encoder.delete_fusion_layer(adapter_names)

    def _delete_switch_layer(self, adapter_names):
        self.encoder.delete_switch_layer(adapter_names)


    def get_fusion_regularization_loss(self):
        reg_loss = 0.0

        target = torch.zeros((self.config.hidden_size, self.config.hidden_size)).fill_diagonal_(1.0).to(self.device)
        for _, v in self.encoder.layer._modules.items():

            for _, layer_fusion in v.output.adapter_fusion_layer.items():
                if hasattr(layer_fusion, "value"):
                    reg_loss += 0.01 * (target - layer_fusion.value.weight).pow(2).sum()

            for _, layer_fusion in v.attention.output.adapter_fusion_layer.items():
                if hasattr(layer_fusion, "value"):
                    reg_loss += 0.01 * (target - layer_fusion.value.weight).pow(2).sum()
        return reg_loss

    def get_adapter(self, name):
        return_adapters = {}
        for idx, layer in enumerate(self.encoder.layer):
            adapters = {
                "attention": layer.attention.output.adapters,
                "output": layer.output.adapters,
            }
            for key, adapt in adapters.items():
                if hasattr(adapt, name):
                    if idx not in return_adapters:
                        return_adapters[idx] = {}
                    return_adapters[idx][key] = getattr(adapt, name)

        return return_adapters


class BertModelHeadsMixin(ModelWithFlexibleHeadsAdaptersMixin):
    """
    Adds flexible heads to a BERT-based model class.
    """

    head_types = {
        "classification": ClassificationHead,
        "multilabel_classification": MultiLabelClassificationHead,
        "tagging": TaggingHead,
        "multiple_choice": MultipleChoiceHead,
        "question_answering": QuestionAnsweringHead,
        "dependency_parsing": BiaffineParsingHead,
        "masked_lm": BertStyleMaskedLMHead,
        "causal_lm": CausalLMHead,
    }

    def add_classification_head(
        self,
        head_name,
        num_labels=2,
        layers=2,
        activation_function="tanh",
        overwrite_ok=False,
        multilabel=False,
        id2label=None,
        use_pooler=False,
    ):
        """
        Adds a sequence classification head on top of the model.

        Args:
            head_name (str): The name of the head.
            num_labels (int, optional): Number of classification labels. Defaults to 2.
            layers (int, optional): Number of layers. Defaults to 2.
            activation_function (str, optional): Activation function. Defaults to 'tanh'.
            overwrite_ok (bool, optional): Force overwrite if a head with the same name exists. Defaults to False.
            multilabel (bool, optional): Enable multilabel classification setup. Defaults to False.
        """

        if multilabel:
            head = MultiLabelClassificationHead(
                self, head_name, num_labels, layers, activation_function, id2label, use_pooler
            )
        else:
            head = ClassificationHead(self, head_name, num_labels, layers, activation_function, id2label, use_pooler)
        self.add_prediction_head(head, overwrite_ok)

    def add_multiple_choice_head(
        self,
        head_name,
        num_choices=2,
        layers=2,
        activation_function="tanh",
        overwrite_ok=False,
        id2label=None,
        use_pooler=False,
    ):
        """
        Adds a multiple choice head on top of the model.

        Args:
            head_name (str): The name of the head.
            num_choices (int, optional): Number of choices. Defaults to 2.
            layers (int, optional): Number of layers. Defaults to 2.
            activation_function (str, optional): Activation function. Defaults to 'tanh'.
            overwrite_ok (bool, optional): Force overwrite if a head with the same name exists. Defaults to False.
        """
        head = MultipleChoiceHead(self, head_name, num_choices, layers, activation_function, id2label, use_pooler)
        self.add_prediction_head(head, overwrite_ok)

    def add_tagging_head(
        self, head_name, num_labels=2, layers=1, activation_function="tanh", overwrite_ok=False, id2label=None
    ):
        """
        Adds a token classification head on top of the model.

        Args:
            head_name (str): The name of the head.
            num_labels (int, optional): Number of classification labels. Defaults to 2.
            layers (int, optional): Number of layers. Defaults to 1.
            activation_function (str, optional): Activation function. Defaults to 'tanh'.
            overwrite_ok (bool, optional): Force overwrite if a head with the same name exists. Defaults to False.
        """
        head = TaggingHead(self, head_name, num_labels, layers, activation_function, id2label)
        self.add_prediction_head(head, overwrite_ok)

    def add_qa_head(
        self, head_name, num_labels=2, layers=1, activation_function="tanh", overwrite_ok=False, id2label=None
    ):
        head = QuestionAnsweringHead(self, head_name, num_labels, layers, activation_function, id2label)
        self.add_prediction_head(head, overwrite_ok)

    def add_dependency_parsing_head(self, head_name, num_labels=2, overwrite_ok=False, id2label=None):
        """Adds a biaffine dependency parsing head on top of the model.
        The parsing head uses the architecture described in "Is Supervised Syntactic Parsing Beneficial for Language Understanding?
        An Empirical Investigation" (Glavaš & Vulić, 2021) (https://arxiv.org/pdf/2008.06788.pdf).

        Args:
            head_name (str): The name of the head.
            num_labels (int, optional): Number of labels. Defaults to 2.
            overwrite_ok (bool, optional): Force overwrite if a head with the same name exists. Defaults to False.
            id2label (dict, optional): Mapping from label ids to labels. Defaults to None.
        """
        head = BiaffineParsingHead(self, head_name, num_labels, id2label)
        self.add_prediction_head(head, overwrite_ok)

    def add_masked_lm_head(self, head_name, activation_function="gelu", overwrite_ok=False):
        """Adds a masked language modeling head on top of the model.

        Args:
            head_name (str): The name of the head.
            activation_function (str, optional): Activation function. Defaults to 'gelu'.
            overwrite_ok (bool, optional): Force overwrite if a head with the same name exists. Defaults to False.
        """
        head = BertStyleMaskedLMHead(self, head_name, activation_function=activation_function)
        self.add_prediction_head(head, overwrite_ok=overwrite_ok)

    def add_causal_lm_head(self, head_name, activation_function="gelu", overwrite_ok=False):
        """Adds a causal language modeling head on top of the model.

        Args:
            head_name (str): The name of the head.
            activation_function (str, optional): Activation function. Defaults to 'gelu'.
            overwrite_ok (bool, optional): Force overwrite if a head with the same name exists. Defaults to False.
        """
        head = CausalLMHead(
            self, head_name, layers=2, activation_function=activation_function, layer_norm=True, bias=True
        )
        self.add_prediction_head(head, overwrite_ok=overwrite_ok)
